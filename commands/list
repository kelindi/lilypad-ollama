#!/bin/bash

echo "Complete Ollama Model Library:"
echo ""
echo "# Large Models (>100B)"
echo "  - deepseek-v3:671b"
echo "  - llama3.1:405b"
echo "  - deepseek-coder-v2:236b"
echo "  - deepseek-v2:236b"
echo "  - dbrx:132b"
echo "  - mistral-large:123b"
echo "  - megadolphin:120b"
echo "  - command-r-plus:104b"
echo ""
echo "# Mixture of Experts Models"
echo "  - mixtral:8x7b,8x22b"
echo "  - dolphin-mixtral:8x7b,8x22b"
echo "  - wizardlm2:8x22b"
echo ""
echo "# Mid-Size Models (30-100B)"
echo "  - llama2:70b"
echo "  - llama3:70b"
echo "  - codellama:70b"
echo "  - qwen:72b"
echo "  - qwen2:72b"
echo "  - deepseek-llm:67b"
echo "  - falcon:40b"
echo "  - command-r:35b"
echo "  - llava:34b"
echo "  - phind-codellama:34b"
echo ""
echo "# Standard Models (7-30B)"
echo "  - llama2:7b,13b"
echo "  - mistral:7b"
echo "  - codellama:7b,13b"
echo "  - phi4:14b"
echo "  - qwen:7b,14b"
echo "  - qwen2:7b"
echo "  - stablelm2:12b"
echo "  - mistral-nemo:12b"
echo "  - solar:10.7b"
echo "  - yi:9b"
echo "  - gemma:7b"
echo ""
echo "# Small Models (<7B)"
echo "  - phi:2.7b"
echo "  - phi3:3.8b"
echo "  - orca-mini:3b"
echo "  - llama3.2:1b,3b"
echo "  - tinyllama:1.1b"
echo "  - qwen:0.5b,1.8b,4b"
echo "  - gemma:2b"
echo "  - smollm:135m,360m,1.7b"
echo ""
echo "# Vision Models"
echo "  - llava:7b,13b,34b"
echo "  - llava-phi3:3.8b"
echo "  - bakllava:7b"
echo "  - moondream:1.8b"
echo "  - llama3.2-vision:11b,90b"
echo ""
echo "# Code-Specific Models"
echo "  - codellama:7b,13b,34b,70b"
echo "  - deepseek-coder:1.3b,6.7b,33b"
echo "  - qwen2.5-coder:0.5b-32b"
echo "  - starcoder2:3b,7b,15b"
echo "  - stable-code:3b"
echo "  - sqlcoder:7b,15b"
echo ""
echo "# Embedding Models"
echo "  - nomic-embed-text"
echo "  - mxbai-embed-large:335m"
echo "  - bge-m3:567m"
echo "  - all-minilm:22m,33m"
echo "  - snowflake-arctic-embed:22m-335m"
echo ""
echo "For complete details and updates, visit: https://ollama.ai/library" 